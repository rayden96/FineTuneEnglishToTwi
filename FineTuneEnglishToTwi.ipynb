{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d50bd82cc83e4c2b9eca0991b0508df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33d1c1324959429c8da61f4c64b6b848",
              "IPY_MODEL_770625a015784450b4a96f5f7ddfe1c3",
              "IPY_MODEL_76b026272f3f4850b32f3ad372a495ab"
            ],
            "layout": "IPY_MODEL_fa0f5135fdbd4f61940d87a70ead2f44"
          }
        },
        "33d1c1324959429c8da61f4c64b6b848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a912bbb5dd74e969fa94ba0e3c50237",
            "placeholder": "​",
            "style": "IPY_MODEL_d16899d1937d41bd8a9ea37341993af2",
            "value": "Map: 100%"
          }
        },
        "770625a015784450b4a96f5f7ddfe1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9e590d2594d4e1499bd7a59b98c28da",
            "max": 12436,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa97f67a14d54c7ab0080628b552c1e4",
            "value": 12436
          }
        },
        "76b026272f3f4850b32f3ad372a495ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43d1919dc2224b0cbb419e322076d38b",
            "placeholder": "​",
            "style": "IPY_MODEL_949c6caf7e7648729fbe611eaee0041b",
            "value": " 12436/12436 [00:02&lt;00:00, 5437.14 examples/s]"
          }
        },
        "fa0f5135fdbd4f61940d87a70ead2f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a912bbb5dd74e969fa94ba0e3c50237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d16899d1937d41bd8a9ea37341993af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9e590d2594d4e1499bd7a59b98c28da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa97f67a14d54c7ab0080628b552c1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43d1919dc2224b0cbb419e322076d38b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "949c6caf7e7648729fbe611eaee0041b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5783a1295231427ebe20d835e5c9e887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02801f341c2c41b789eac1bec1e74b06",
              "IPY_MODEL_025be5fe9c0d453cb955b6ebf31c20be",
              "IPY_MODEL_ba150ea973f34eb29923f57c66fe6d9f"
            ],
            "layout": "IPY_MODEL_4861689050784fc494ec7f453cf583b4"
          }
        },
        "02801f341c2c41b789eac1bec1e74b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66c5600ef1043c593c1deb1a8a5b150",
            "placeholder": "​",
            "style": "IPY_MODEL_f9a298f249c54910ac5abec033e6c40e",
            "value": "Map: 100%"
          }
        },
        "025be5fe9c0d453cb955b6ebf31c20be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e36e9cb544d24385a3649a8e6d2d2374",
            "max": 655,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb782bfd13ce4c889da59da452be3e69",
            "value": 655
          }
        },
        "ba150ea973f34eb29923f57c66fe6d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a16a254f9ce94b5ea42cdf702cc4b2e4",
            "placeholder": "​",
            "style": "IPY_MODEL_f587df8f80784c8baefd5dcbdb810bb1",
            "value": " 655/655 [00:00&lt;00:00, 5206.72 examples/s]"
          }
        },
        "4861689050784fc494ec7f453cf583b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66c5600ef1043c593c1deb1a8a5b150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9a298f249c54910ac5abec033e6c40e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e36e9cb544d24385a3649a8e6d2d2374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb782bfd13ce4c889da59da452be3e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a16a254f9ce94b5ea42cdf702cc4b2e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f587df8f80784c8baefd5dcbdb810bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import evaluate\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "fCZyGA0p4TOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-tw\")\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-tw\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a90b20dshLol",
        "outputId": "c06770b9-ee99-4ea1-e3a0-3c7879148df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: from twi_dict.csv, there are english words and one or many twi translations, seperated by a semi colon. read in the dataset, and ensure every word only has one translation, if there are more than one add a new entry with the other translation and remove the other translation from the original entry\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('twi_dict.csv')\n",
        "\n",
        "# Create a new DataFrame to store the updated data\n",
        "new_df = pd.DataFrame(columns=['English', 'Twi'])\n",
        "\n",
        "# Iterate over the rows in the original DataFrame\n",
        "for index, row in df.iterrows():\n",
        "  english_word = row['english']\n",
        "  twi_translations = row['twi'].split(';')\n",
        "\n",
        "  # If there's only one translation, add it to the new DataFrame\n",
        "  if len(twi_translations) == 1:\n",
        "    new_df = pd.concat([new_df, pd.DataFrame({'English': [english_word], 'Twi': [twi_translations[0]]})], ignore_index=True)\n",
        "\n",
        "  # If there are multiple translations, create new entries for each\n",
        "  else:\n",
        "    for translation in twi_translations:\n",
        "      new_df = pd.concat([new_df, pd.DataFrame({'English': [english_word], 'Twi': [translation]})], ignore_index=True)\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(new_df)\n",
        "\n",
        "#remove any entries containing ints or floats\n",
        "new_df = new_df[~new_df['English'].str.contains(r'\\d')]\n",
        "new_df = new_df[~new_df['Twi'].str.contains(r'\\d')]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKSrY6IHthTl",
        "outputId": "2d1d943c-0c44-453b-8658-633d80cbaab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          English                     Twi\n",
            "0        hopeless         anidasoɔ nni mu\n",
            "1        hopeless    deɛ anidasoɔ nni mu \n",
            "2         horizon  ewiem ne asase ahyiaeɛ\n",
            "3            horn                    abɛn\n",
            "4            horn                 abebɛn \n",
            "...           ...                     ...\n",
            "8314  interchange                di nsesa\n",
            "8315     interior                     emu\n",
            "8316     keenness                   aniku\n",
            "8317    interlude                ntwaremu\n",
            "8318    interfere          twitwa anan mu\n",
            "\n",
            "[8319 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: add an id which is a string id\n",
        "\n",
        "import uuid\n",
        "\n",
        "# Generate unique IDs for each row\n",
        "new_df['id'] = [str(uuid.uuid4()) for _ in range(len(new_df))]\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(new_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VfuDfvpvvPl",
        "outputId": "a4c73a16-574a-49c4-cbb7-a24fcade93ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        id      English  \\\n",
            "0     ff754d39-e039-4bf1-bcf3-2bec77e2d55d     hopeless   \n",
            "1     5fc00867-ca84-4c5d-9aa6-0b814f478297     hopeless   \n",
            "2     c132e785-e0dd-40c9-b2c9-ea31c55020d0      horizon   \n",
            "3     1244c350-5482-4c78-b5a9-05aaecac1272         horn   \n",
            "4     21a73391-9365-44df-b4c4-ec5013fd650b         horn   \n",
            "...                                    ...          ...   \n",
            "8314  2fc951dd-2ef9-43f9-a145-0c5637cd5941  interchange   \n",
            "8315  6262ece2-2540-492e-9a29-d0a133ba8f0e     interior   \n",
            "8316  cc527a98-b569-483b-a573-4a116670e631     keenness   \n",
            "8317  658b2de2-53c0-4b87-aac5-daa507f91124    interlude   \n",
            "8318  11fd016b-57b2-46b2-90b7-bbaaffc4edc3    interfere   \n",
            "\n",
            "                         Twi  \n",
            "0            anidasoɔ nni mu  \n",
            "1       deɛ anidasoɔ nni mu   \n",
            "2     ewiem ne asase ahyiaeɛ  \n",
            "3                       abɛn  \n",
            "4                    abebɛn   \n",
            "...                      ...  \n",
            "8314                di nsesa  \n",
            "8315                     emu  \n",
            "8316                   aniku  \n",
            "8317                ntwaremu  \n",
            "8318          twitwa anan mu  \n",
            "\n",
            "[8291 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_df = pd.read_csv('MetaStuff/Train.csv')\n",
        "\n",
        "import re\n",
        "\n",
        "#add new df to train_df\n",
        "train_df = pd.concat([train_df, new_df], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "# Function to normalize text\n",
        "def normalize_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters, keeping only alphanumeric and basic punctuation\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
        "\n",
        "    # Remove multiple spaces and trim\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply normalization to both English and Twi columns\n",
        "train_df['English'] = train_df['English'].apply(normalize_text)\n",
        "train_df['Twi'] = train_df['Twi'].apply(normalize_text)\n",
        "\n",
        "# Show a few examples of normalized data\n",
        "print(train_df[['English', 'Twi']].head())\n",
        "\n",
        "# Save the normalized dataset if needed\n",
        "# train_df.to_csv('train_normalized.csv', index=False)\n",
        "\n",
        "\n",
        "# Convert it to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "# Display a few records to ensure it's loaded correctly\n",
        "train_dataset = train_dataset.rename_column('English', 'source')\n",
        "train_dataset = train_dataset.rename_column('Twi', 'target')\n",
        "print(train_dataset)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_test_split = train_dataset.train_test_split(test_size=0.05)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFqwT4QZ4V6f",
        "outputId": "bfa80d03-e4a7-4a6c-ca77-de126a44f3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             English  \\\n",
            "0           how best can we promote sports in ghana?   \n",
            "1  challenging programs often call for a large am...   \n",
            "2  they usually go into agreement that the first ...   \n",
            "3                it enables children to grow up well   \n",
            "4  education in africa is lagging behind because ...   \n",
            "\n",
            "                                                 Twi  \n",
            "0  kwan bn so na yebetumi ahy agumadi ho nkuran y...  \n",
            "1  mpn pii no, nhyehye ahorow a emu y den hwehw s...  \n",
            "2  mpn pii no, y a na wy adwene kor s de bdi kan ...  \n",
            "3                            ma mmfra tumi nyini yie  \n",
            "4  nhomasua a w abibiman mu aka nakyi esiane s su...  \n",
            "Dataset({\n",
            "    features: ['id', 'source', 'target'],\n",
            "    num_rows: 13091\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_name = \"facebook/m2m100_418M\"\n",
        "#tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the language codes\n",
        "tokenizer.src_lang = \"en\"  # English\n",
        "tokenizer.tgt_lang = \"tn\"  # Twi\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['source']\n",
        "    targets = examples['target']\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Tokenize the targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to dataset\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True, load_from_cache_file=False)\n",
        "eval_dataset = eval_dataset.map(preprocess_function, batched=True, load_from_cache_file=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "d50bd82cc83e4c2b9eca0991b0508df4",
            "33d1c1324959429c8da61f4c64b6b848",
            "770625a015784450b4a96f5f7ddfe1c3",
            "76b026272f3f4850b32f3ad372a495ab",
            "fa0f5135fdbd4f61940d87a70ead2f44",
            "0a912bbb5dd74e969fa94ba0e3c50237",
            "d16899d1937d41bd8a9ea37341993af2",
            "f9e590d2594d4e1499bd7a59b98c28da",
            "fa97f67a14d54c7ab0080628b552c1e4",
            "43d1919dc2224b0cbb419e322076d38b",
            "949c6caf7e7648729fbe611eaee0041b",
            "5783a1295231427ebe20d835e5c9e887",
            "02801f341c2c41b789eac1bec1e74b06",
            "025be5fe9c0d453cb955b6ebf31c20be",
            "ba150ea973f34eb29923f57c66fe6d9f",
            "4861689050784fc494ec7f453cf583b4",
            "f66c5600ef1043c593c1deb1a8a5b150",
            "f9a298f249c54910ac5abec033e6c40e",
            "e36e9cb544d24385a3649a8e6d2d2374",
            "eb782bfd13ce4c889da59da452be3e69",
            "a16a254f9ce94b5ea42cdf702cc4b2e4",
            "f587df8f80784c8baefd5dcbdb810bb1"
          ]
        },
        "id": "io0k1uXc4bwu",
        "outputId": "6e15947e-da8f-4607-9655-542687abaabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/12436 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d50bd82cc83e4c2b9eca0991b0508df4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/655 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5783a1295231427ebe20d835e5c9e887"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for padding\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Load the ROUGE metric for evaluation\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(pred.strip() for pred in decoded_pred.split()) for decoded_pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(label.strip() for label in decoded_label.split()) for decoded_label in decoded_labels]\n",
        "\n",
        "    # Compute ROUGE\n",
        "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # Since the result is already in float format, we just multiply by 100\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "AQbZLshq4fwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to fine tune the model on the training dataset\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results_new\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=4,  # Adjust based on your memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=3e-5,\n",
        "    num_train_epochs=10,  # Adjust the number of epochs\n",
        "    predict_with_generate=True,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_dir='./logs_new',\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rouge1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,\n",
        "    lr_scheduler_type=\"cosine\",  # or \"linear\"\n",
        ")\n",
        "\n",
        "# Create the trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7xvhS7zi2ZE",
        "outputId": "24d52c32-50b0-4ed1-f225-e53f13ad94ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./trained_model_new\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VDt_jrTGi9i0",
        "outputId": "e0f50a01-0bae-42ed-e0a5-83d10f28940b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11037' max='15540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11037/15540 49:39 < 20:15, 3.70 it/s, Epoch 7.10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.328600</td>\n",
              "      <td>0.320402</td>\n",
              "      <td>23.644055</td>\n",
              "      <td>7.941135</td>\n",
              "      <td>22.228434</td>\n",
              "      <td>23.678144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.308700</td>\n",
              "      <td>0.282580</td>\n",
              "      <td>26.309214</td>\n",
              "      <td>9.822873</td>\n",
              "      <td>24.840863</td>\n",
              "      <td>26.282028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.299500</td>\n",
              "      <td>0.264141</td>\n",
              "      <td>28.067404</td>\n",
              "      <td>11.435534</td>\n",
              "      <td>26.668755</td>\n",
              "      <td>28.117659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.243900</td>\n",
              "      <td>0.253542</td>\n",
              "      <td>29.762264</td>\n",
              "      <td>12.431210</td>\n",
              "      <td>28.201075</td>\n",
              "      <td>29.779861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.246400</td>\n",
              "      <td>0.243546</td>\n",
              "      <td>31.262493</td>\n",
              "      <td>13.442960</td>\n",
              "      <td>29.704420</td>\n",
              "      <td>31.207889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.232700</td>\n",
              "      <td>0.235968</td>\n",
              "      <td>31.449403</td>\n",
              "      <td>13.919467</td>\n",
              "      <td>30.018372</td>\n",
              "      <td>31.436735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.208900</td>\n",
              "      <td>0.232874</td>\n",
              "      <td>32.102685</td>\n",
              "      <td>14.564479</td>\n",
              "      <td>30.672319</td>\n",
              "      <td>32.084213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.201800</td>\n",
              "      <td>0.228625</td>\n",
              "      <td>32.984790</td>\n",
              "      <td>15.233581</td>\n",
              "      <td>31.699176</td>\n",
              "      <td>32.987604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.193500</td>\n",
              "      <td>0.225862</td>\n",
              "      <td>32.143572</td>\n",
              "      <td>14.940347</td>\n",
              "      <td>30.879241</td>\n",
              "      <td>32.086860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.172100</td>\n",
              "      <td>0.224042</td>\n",
              "      <td>33.699899</td>\n",
              "      <td>15.645748</td>\n",
              "      <td>32.531750</td>\n",
              "      <td>33.669195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.178300</td>\n",
              "      <td>0.221839</td>\n",
              "      <td>33.894359</td>\n",
              "      <td>15.760270</td>\n",
              "      <td>32.640683</td>\n",
              "      <td>33.873729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.177000</td>\n",
              "      <td>0.219237</td>\n",
              "      <td>34.015512</td>\n",
              "      <td>15.786005</td>\n",
              "      <td>32.866862</td>\n",
              "      <td>33.985439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.167700</td>\n",
              "      <td>0.218116</td>\n",
              "      <td>34.610528</td>\n",
              "      <td>16.115614</td>\n",
              "      <td>33.366495</td>\n",
              "      <td>34.489345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.142800</td>\n",
              "      <td>0.218380</td>\n",
              "      <td>35.113565</td>\n",
              "      <td>15.755334</td>\n",
              "      <td>33.707452</td>\n",
              "      <td>35.096339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.149500</td>\n",
              "      <td>0.216756</td>\n",
              "      <td>33.999493</td>\n",
              "      <td>15.854602</td>\n",
              "      <td>32.570048</td>\n",
              "      <td>33.944287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.132600</td>\n",
              "      <td>0.217275</td>\n",
              "      <td>35.028372</td>\n",
              "      <td>15.916115</td>\n",
              "      <td>33.820733</td>\n",
              "      <td>34.920852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.139100</td>\n",
              "      <td>0.217277</td>\n",
              "      <td>35.067635</td>\n",
              "      <td>16.457099</td>\n",
              "      <td>33.599671</td>\n",
              "      <td>34.949570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.130600</td>\n",
              "      <td>0.216732</td>\n",
              "      <td>35.340790</td>\n",
              "      <td>16.364116</td>\n",
              "      <td>34.020476</td>\n",
              "      <td>35.234738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.135800</td>\n",
              "      <td>0.216463</td>\n",
              "      <td>35.431107</td>\n",
              "      <td>16.534055</td>\n",
              "      <td>34.207630</td>\n",
              "      <td>35.343670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.127600</td>\n",
              "      <td>0.216480</td>\n",
              "      <td>35.364352</td>\n",
              "      <td>16.492218</td>\n",
              "      <td>33.994078</td>\n",
              "      <td>35.336821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.123600</td>\n",
              "      <td>0.216898</td>\n",
              "      <td>34.895905</td>\n",
              "      <td>16.033030</td>\n",
              "      <td>33.709055</td>\n",
              "      <td>34.867831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.116300</td>\n",
              "      <td>0.216821</td>\n",
              "      <td>35.504268</td>\n",
              "      <td>16.332867</td>\n",
              "      <td>34.176977</td>\n",
              "      <td>35.424120</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[56999]], 'forced_eos_token_id': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Predict the Test dataset and store the predicitions in a csv called KnownModelPreds\n",
        "import os\n",
        "# Load the test set\n",
        "test_df = pd.read_csv(\"MetaStuff/Test.csv\")\n",
        "\n",
        "# Ensure the test set has the correct columns\n",
        "assert 'id' in test_df.columns and 'English' in test_df.columns, \"Test set must contain 'id' and 'English' columns.\"\n",
        "\n",
        "# Prepare the tokenizer and model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize the output CSV file\n",
        "output_file = \"KnownModelPreds.csv\"\n",
        "\n",
        "# If the output file doesn't exist, create it and write the header\n",
        "if not os.path.exists(output_file):\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(\"id,Twi\\n\")  # Writing the header\n",
        "\n",
        "# Tokenize the English sentences\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "# Function to clean and format the translation\n",
        "def clean_translation(translation):\n",
        "    # Remove any commas in the translation\n",
        "    translation = translation.replace(\",\", \"\")\n",
        "    # Return the cleaned translation with quotes\n",
        "    return translation.strip()\n",
        "\n",
        "# Process the data in batches and save the output to CSV after every batch\n",
        "batch_size = 16  # Adjust batch size based on memory capacity\n",
        "for i in range(0, len(test_df), batch_size):\n",
        "    batch = test_df.iloc[i:i + batch_size]\n",
        "\n",
        "    # Tokenize the English sentences\n",
        "    inputs = tokenize_function(batch[\"English\"].tolist())\n",
        "\n",
        "    # Move inputs to the correct device\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Generate translations using the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Decode the predictions into Twi translations\n",
        "    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Write the results to CSV after every batch\n",
        "    with open(output_file, \"a\") as f:\n",
        "        for j in range(len(decoded_preds)):\n",
        "            # Clean the translation and write to the file\n",
        "            #cleaned_translation = clean_translation(decoded_preds[j])\n",
        "            f.write(f\"{batch['id'].iloc[j]},{decoded_preds[j]}\\n\")\n",
        "\n",
        "    print(f\"Processed batch {i // batch_size + 1}/{len(test_df) // batch_size + 1}\")\n",
        "\n",
        "print(f\"Translations saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLzLN26DhQXd",
        "outputId": "b898799c-7fef-4b20-ad00-c95891253f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1/201\n",
            "Processed batch 2/201\n",
            "Processed batch 3/201\n",
            "Processed batch 4/201\n",
            "Processed batch 5/201\n",
            "Processed batch 6/201\n",
            "Processed batch 7/201\n",
            "Processed batch 8/201\n",
            "Processed batch 9/201\n",
            "Processed batch 10/201\n",
            "Processed batch 11/201\n",
            "Processed batch 12/201\n",
            "Processed batch 13/201\n",
            "Processed batch 14/201\n",
            "Processed batch 15/201\n",
            "Processed batch 16/201\n",
            "Processed batch 17/201\n",
            "Processed batch 18/201\n",
            "Processed batch 19/201\n",
            "Processed batch 20/201\n",
            "Processed batch 21/201\n",
            "Processed batch 22/201\n",
            "Processed batch 23/201\n",
            "Processed batch 24/201\n",
            "Processed batch 25/201\n",
            "Processed batch 26/201\n",
            "Processed batch 27/201\n",
            "Processed batch 28/201\n",
            "Processed batch 29/201\n",
            "Processed batch 30/201\n",
            "Processed batch 31/201\n",
            "Processed batch 32/201\n",
            "Processed batch 33/201\n",
            "Processed batch 34/201\n",
            "Processed batch 35/201\n",
            "Processed batch 36/201\n",
            "Processed batch 37/201\n",
            "Processed batch 38/201\n",
            "Processed batch 39/201\n",
            "Processed batch 40/201\n",
            "Processed batch 41/201\n",
            "Processed batch 42/201\n",
            "Processed batch 43/201\n",
            "Processed batch 44/201\n",
            "Processed batch 45/201\n",
            "Processed batch 46/201\n",
            "Processed batch 47/201\n",
            "Processed batch 48/201\n",
            "Processed batch 49/201\n",
            "Processed batch 50/201\n",
            "Processed batch 51/201\n",
            "Processed batch 52/201\n",
            "Processed batch 53/201\n",
            "Processed batch 54/201\n",
            "Processed batch 55/201\n",
            "Processed batch 56/201\n",
            "Processed batch 57/201\n",
            "Processed batch 58/201\n",
            "Processed batch 59/201\n",
            "Processed batch 60/201\n",
            "Processed batch 61/201\n",
            "Processed batch 62/201\n",
            "Processed batch 63/201\n",
            "Processed batch 64/201\n",
            "Processed batch 65/201\n",
            "Processed batch 66/201\n",
            "Processed batch 67/201\n",
            "Processed batch 68/201\n",
            "Processed batch 69/201\n",
            "Processed batch 70/201\n",
            "Processed batch 71/201\n",
            "Processed batch 72/201\n",
            "Processed batch 73/201\n",
            "Processed batch 74/201\n",
            "Processed batch 75/201\n",
            "Processed batch 76/201\n",
            "Processed batch 77/201\n",
            "Processed batch 78/201\n",
            "Processed batch 79/201\n",
            "Processed batch 80/201\n",
            "Processed batch 81/201\n",
            "Processed batch 82/201\n",
            "Processed batch 83/201\n",
            "Processed batch 84/201\n",
            "Processed batch 85/201\n",
            "Processed batch 86/201\n",
            "Processed batch 87/201\n",
            "Processed batch 88/201\n",
            "Processed batch 89/201\n",
            "Processed batch 90/201\n",
            "Processed batch 91/201\n",
            "Processed batch 92/201\n",
            "Processed batch 93/201\n",
            "Processed batch 94/201\n",
            "Processed batch 95/201\n",
            "Processed batch 96/201\n",
            "Processed batch 97/201\n",
            "Processed batch 98/201\n",
            "Processed batch 99/201\n",
            "Processed batch 100/201\n",
            "Processed batch 101/201\n",
            "Processed batch 102/201\n",
            "Processed batch 103/201\n",
            "Processed batch 104/201\n",
            "Processed batch 105/201\n",
            "Processed batch 106/201\n",
            "Processed batch 107/201\n",
            "Processed batch 108/201\n",
            "Processed batch 109/201\n",
            "Processed batch 110/201\n",
            "Processed batch 111/201\n",
            "Processed batch 112/201\n",
            "Processed batch 113/201\n",
            "Processed batch 114/201\n",
            "Processed batch 115/201\n",
            "Processed batch 116/201\n",
            "Processed batch 117/201\n",
            "Processed batch 118/201\n",
            "Processed batch 119/201\n",
            "Processed batch 120/201\n",
            "Processed batch 121/201\n",
            "Processed batch 122/201\n",
            "Processed batch 123/201\n",
            "Processed batch 124/201\n",
            "Processed batch 125/201\n",
            "Processed batch 126/201\n",
            "Processed batch 127/201\n",
            "Processed batch 128/201\n",
            "Processed batch 129/201\n",
            "Processed batch 130/201\n",
            "Processed batch 131/201\n",
            "Processed batch 132/201\n",
            "Processed batch 133/201\n",
            "Processed batch 134/201\n",
            "Processed batch 135/201\n",
            "Processed batch 136/201\n",
            "Processed batch 137/201\n",
            "Processed batch 138/201\n",
            "Processed batch 139/201\n",
            "Processed batch 140/201\n",
            "Processed batch 141/201\n",
            "Processed batch 142/201\n",
            "Processed batch 143/201\n",
            "Processed batch 144/201\n",
            "Processed batch 145/201\n",
            "Processed batch 146/201\n",
            "Processed batch 147/201\n",
            "Processed batch 148/201\n",
            "Processed batch 149/201\n",
            "Processed batch 150/201\n",
            "Processed batch 151/201\n",
            "Processed batch 152/201\n",
            "Processed batch 153/201\n",
            "Processed batch 154/201\n",
            "Processed batch 155/201\n",
            "Processed batch 156/201\n",
            "Processed batch 157/201\n",
            "Processed batch 158/201\n",
            "Processed batch 159/201\n",
            "Processed batch 160/201\n",
            "Processed batch 161/201\n",
            "Processed batch 162/201\n",
            "Processed batch 163/201\n",
            "Processed batch 164/201\n",
            "Processed batch 165/201\n",
            "Processed batch 166/201\n",
            "Processed batch 167/201\n",
            "Processed batch 168/201\n",
            "Processed batch 169/201\n",
            "Processed batch 170/201\n",
            "Processed batch 171/201\n",
            "Processed batch 172/201\n",
            "Processed batch 173/201\n",
            "Processed batch 174/201\n",
            "Processed batch 175/201\n",
            "Processed batch 176/201\n",
            "Processed batch 177/201\n",
            "Processed batch 178/201\n",
            "Processed batch 179/201\n",
            "Processed batch 180/201\n",
            "Processed batch 181/201\n",
            "Processed batch 182/201\n",
            "Processed batch 183/201\n",
            "Processed batch 184/201\n",
            "Processed batch 185/201\n",
            "Processed batch 186/201\n",
            "Processed batch 187/201\n",
            "Processed batch 188/201\n",
            "Processed batch 189/201\n",
            "Processed batch 190/201\n",
            "Processed batch 191/201\n",
            "Processed batch 192/201\n",
            "Processed batch 193/201\n",
            "Processed batch 194/201\n",
            "Processed batch 195/201\n",
            "Processed batch 196/201\n",
            "Processed batch 197/201\n",
            "Processed batch 198/201\n",
            "Processed batch 199/201\n",
            "Processed batch 200/201\n",
            "Translations saved to KnownModelPreds.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load KnownModelPreds and replace all \" with nothing\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"KnownModelPreds.csv\")\n",
        "\n",
        "# Replace all occurrences of \" with nothing\n",
        "df['Twi'] = df['Twi'].str.replace('\"', '')\n",
        "\n",
        "# Save the modified DataFrame back to the CSV file\n",
        "df.to_csv('KnownModelPreds.csv', index=False)\n"
      ],
      "metadata": {
        "id": "EriP2WPLnlog"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}